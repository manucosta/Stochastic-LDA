{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Variational Inference for LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el presente notebook se realiza una implementación del algoritmo para topic modelling, Latent Dirichlet Allocation (LDA), en su versión con Stochastic Variational Inference. La implementación trata de apegarse a lo propuesto por Hoffman, Blei y Wang en [Stochastic Variational Inference](http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf).\n",
    "Otros trabajos que fueron tomados como referencia son:\n",
    " * Para un panorama más amplio sobre LDA: [Inference Methods for Latent Dirichlet Allocation](http://times.cs.uiuc.edu/course/598f16/notes/lda-survey.pdf);\n",
    " * Para una explicación más detallada sobre la Mean-Field Variational Family: [Variational Inference: A Review for Statisticians](https://arxiv.org/pdf/1601.00670.pdf)\n",
    " \n",
    "Adicionalmente, la idea de implementar LDA surgió de un intento previo de querer implementar la propuesta de Wang y Blei, en [Collaborative Topic Modeling for Recommending Scientific Articles](http://www.cs.columbia.edu/~blei/papers/WangBlei2011.pdf), para Sistemas de Recomendación. De hecho el dataset utilizado aqui es el mismo que usan ellos. No descarto como siguiente paso intentar un Content-Based Recommender System."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marco teórico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Generativo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asumiendo K tópicos, y D documentos, cada uno con N palabras (por simplicidad asumo documentos de igual longitud) pertenecientes a un vocabulario de tamaño V; el modelo generativo es el siguiente:\n",
    "\n",
    "1. Generamos tópicos $\\beta_k$ ~ Dirichlet($\\eta,\\dots,\\eta$) para cada $k\\in \\{0,\\dots,K-1\\}$\n",
    "2. Para cada documento $d \\in \\{0,\\dots, D-1\\}$:\n",
    "    3. Generamos $\\theta_d$ ~ Dirichlet($\\alpha,\\dots,\\alpha$)\n",
    "    4. Para cada palabra $w \\in \\{0,\\dots, N-1\\}$:\n",
    "        5. Generamos la asignación de tema $z_{dn}$ ~ Multinomial($\\theta_d$)\n",
    "        6. Generamos la palabra $w_{dn}$ ~ Multinomial($\\beta_{z_{dn}}$)\n",
    "\n",
    "Los parámetros involucrados son:\n",
    "* $\\beta_k$, el tópico $k$. Un tópico consiste básicamente en un vector de probabilidad de longitud V, que modela la distribución de las distintas palabras en el vocabulario para ese tema\n",
    "* $\\theta_d$, la proporción de los tópicos para el documento $d$. Nuevamente es un vector de probabilidades, que en este caso apunta a modelar la participación parcial de los distintos temas para un mismo documento. El hecho de que se permita que un documento pertenezca a un mixture de tópicos en lugar de uno solo, es una de las principales virtudes de LDA. __Esta es la principal variable sobre la que nos interesa poder hacer inferencia__.\n",
    "* $z_{dn}$, es la asignación del tópico de la palabra $n$ en el domunento $d$. Por cuestiones prácticas es un one-hot vector K-dimensional\n",
    "* $w_{dn}$, es la palabra $n$ del documento $d$. Pertenece al tópico $z_{dn}$. Por cuestiones prácticas es un one-hot vector V-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/lda-graphical-model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para estimar las distribuciones de arriba se utilizan las siguientes familias variacionales:\n",
    "* Para las asignaciones de tópicos, usamos q($z_{dn}$) ~ Multinomial($\\phi_{dn}$)\n",
    "* Para las proporciones de tópicos en cada documentos, q($\\theta_{d}$) ~ Dirichlet($\\gamma_{d}$)\n",
    "* Para las distribuciones de probabilidad sobre las palabras de cada tópico, q($\\beta_{k}$) ~ Dirichlet($\\lambda_{k}$)\n",
    "\n",
    "En particular, $\\gamma$ y $\\lambda$ son los parámetros de mayor interés como output del modelo, ya que el primero es el que nos permite tener la distribución de tópicos para un documento visto, mientras que el segundo nos permite hacer la inferencia de estas proporciones sobre nuevos documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El dataset fue descargado de http://www.cs.cmu.edu/~chongw/data/citeulike/. El mismo contiene información del sitio http://www.citeulike.org/, que le permite a investigadores armar sus bibliotecas de papers y recibir recomendaciones en base a las mismas. En particular yo no uso la información referente a los usuarios y sus bibliotecas, si no que únicamente me centro en la data sobre artículos.\n",
    "Los dos archivos que uso son:\n",
    "* mult.dat, que contiene los ids de las palabras más relevantes de cada artículos, y sus conteos. En total son 16980 artículos (documentos).\n",
    "* vocab.dat, el mapeo de los ids a la palabra específica. El tamaño del vocabulario usado es de 8000 palabras.\n",
    "\n",
    "La extracción del vocabulario, y demás preprocesamientos de la data son los que se explican en la sección 4 de [Wang and Blei (2011)](http://www.cs.columbia.edu/~blei/papers/WangBlei2011.pdf).\n",
    "\n",
    "Adicionalmente, en esta primera iteración del algoritmo, pido que todos los documentos tengan la misma cantidad de palabras. Eso me lleva a realizar un procesamiento extra, en el cual al cargar los documentos hago un resampling (con reposición) para tener 60 palabras por documento (esto es totalmente arbitrario)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import dirichlet, multinomial, expon\n",
    "from scipy.special import digamma\n",
    "from collections import defaultdict\n",
    "from math import pow\n",
    "import numpy as np\n",
    "import copy\n",
    "import logging\n",
    "\n",
    "from online_lda import OnlineLDA\n",
    "from mini_batch_lda import MiniBatchLDA\n",
    "from helpers import *\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    words_per_doc = defaultdict(list)\n",
    "    # Parse line by line the documents data's file\n",
    "    with open('data/mult.dat') as keywords_f:\n",
    "        words_and_counts_by_doc = []\n",
    "        for l in keywords_f:\n",
    "            data = l.split() # First element is the number of distinct words, the rest are the word_ids with its counters \n",
    "            words_and_counts_by_doc.append(data[1:])\n",
    "        for d, words_and_counts in enumerate(words_and_counts_by_doc):\n",
    "            for w_c in words_and_counts:\n",
    "                word, cnt = w_c.split(':')\n",
    "                for _ in xrange(int(cnt)):    \n",
    "                    words_per_doc[d].append(int(word))\n",
    "            # Resampling the data to always have the same number of words per document\n",
    "            np.random.seed(42)\n",
    "            words_per_doc[d] = np.random.choice(words_per_doc[d], size=60)\n",
    "    \n",
    "    vocab = {}\n",
    "    with open('data/vocab.dat') as vocab_f:\n",
    "        for v, word in enumerate(vocab_f):\n",
    "            vocab[v] = word.rstrip()\n",
    "    \n",
    "    return words_per_doc, vocab\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_doc, vocab = load_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero me defino algunas funciones auxiliares que me van a servir luego."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La clase _LatentDirichletAllocation_ tiene como método principal _fit_, que es el que se encarga de aplicar el algoritmo propuesto por Hoffman et al. (2013). La notación elegida sigue fielmente a la usada en el paper.\n",
    "\n",
    "La imagen siguiente muestra el pseudocódigo del algoritmo en cuestión."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/SVI-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatch LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mini_batch_lda:Iteration: 0\n",
      "INFO:mini_batch_lda:Ro: 1.0\n",
      "INFO:mini_batch_lda:Iteration: 1\n",
      "INFO:mini_batch_lda:Ro: 0.574349177499\n",
      "INFO:mini_batch_lda:Iteration: 2\n",
      "INFO:mini_batch_lda:Ro: 0.415243646539\n",
      "INFO:mini_batch_lda:Iteration: 3\n",
      "INFO:mini_batch_lda:Ro: 0.329876977693\n",
      "INFO:mini_batch_lda:Iteration: 4\n",
      "INFO:mini_batch_lda:Ro: 0.275945932292\n",
      "INFO:mini_batch_lda:Iteration: 5\n",
      "INFO:mini_batch_lda:Ro: 0.238494846851\n",
      "INFO:mini_batch_lda:Iteration: 6\n",
      "INFO:mini_batch_lda:Ro: 0.210824737371\n",
      "INFO:mini_batch_lda:Iteration: 7\n",
      "INFO:mini_batch_lda:Ro: 0.189464570814\n",
      "INFO:mini_batch_lda:Iteration: 8\n",
      "INFO:mini_batch_lda:Ro: 0.172427285991\n",
      "INFO:mini_batch_lda:Iteration: 9\n",
      "INFO:mini_batch_lda:Ro: 0.158489319246\n",
      "INFO:mini_batch_lda:Iteration: 10\n",
      "INFO:mini_batch_lda:Ro: 0.1468540242\n",
      "INFO:mini_batch_lda:Iteration: 11\n",
      "INFO:mini_batch_lda:Ro: 0.136979319126\n",
      "INFO:mini_batch_lda:Iteration: 12\n",
      "INFO:mini_batch_lda:Ro: 0.128482896333\n",
      "INFO:mini_batch_lda:Iteration: 13\n",
      "INFO:mini_batch_lda:Ro: 0.121087014505\n",
      "INFO:mini_batch_lda:Iteration: 14\n",
      "INFO:mini_batch_lda:Ro: 0.114584795172\n",
      "INFO:mini_batch_lda:Iteration: 15\n",
      "INFO:mini_batch_lda:Ro: 0.108818820412\n",
      "INFO:mini_batch_lda:Iteration: 16\n",
      "INFO:mini_batch_lda:Ro: 0.103667079284\n",
      "INFO:mini_batch_lda:Iteration: 17\n",
      "INFO:mini_batch_lda:Ro: 0.099033469887\n",
      "INFO:mini_batch_lda:Iteration: 18\n",
      "INFO:mini_batch_lda:Ro: 0.0948412172272\n",
      "INFO:mini_batch_lda:Iteration: 19\n",
      "INFO:mini_batch_lda:Ro: 0.0910282101513\n",
      "INFO:mini_batch_lda:Iteration: 20\n",
      "INFO:mini_batch_lda:Ro: 0.0875436327263\n",
      "INFO:mini_batch_lda:Iteration: 21\n",
      "INFO:mini_batch_lda:Ro: 0.0843454880117\n",
      "INFO:mini_batch_lda:Iteration: 22\n",
      "INFO:mini_batch_lda:Ro: 0.0813987491546\n",
      "INFO:mini_batch_lda:Iteration: 23\n",
      "INFO:mini_batch_lda:Ro: 0.0786739592746\n",
      "INFO:mini_batch_lda:Iteration: 24\n",
      "INFO:mini_batch_lda:Ro: 0.0761461575486\n",
      "INFO:mini_batch_lda:Iteration: 25\n",
      "INFO:mini_batch_lda:Ro: 0.0737940458317\n",
      "INFO:mini_batch_lda:Iteration: 26\n",
      "INFO:mini_batch_lda:Ro: 0.0715993349975\n",
      "INFO:mini_batch_lda:Iteration: 27\n",
      "INFO:mini_batch_lda:Ro: 0.0695462271868\n",
      "INFO:mini_batch_lda:Iteration: 28\n",
      "INFO:mini_batch_lda:Ro: 0.0676210019812\n",
      "INFO:mini_batch_lda:Iteration: 29\n",
      "INFO:mini_batch_lda:Ro: 0.0658116828612\n",
      "INFO:mini_batch_lda:Iteration: 30\n",
      "INFO:mini_batch_lda:Ro: 0.0641077662795\n",
      "INFO:mini_batch_lda:Iteration: 31\n",
      "INFO:mini_batch_lda:Ro: 0.0625\n",
      "INFO:mini_batch_lda:Iteration: 32\n",
      "INFO:mini_batch_lda:Ro: 0.0609802005177\n",
      "INFO:mini_batch_lda:Iteration: 33\n",
      "INFO:mini_batch_lda:Ro: 0.0595411017206\n",
      "INFO:mini_batch_lda:Iteration: 34\n",
      "INFO:mini_batch_lda:Ro: 0.058176228704\n",
      "INFO:mini_batch_lda:Iteration: 35\n",
      "INFO:mini_batch_lda:Ro: 0.0568797919744\n",
      "INFO:mini_batch_lda:Iteration: 36\n",
      "INFO:mini_batch_lda:Ro: 0.0556465982832\n",
      "INFO:mini_batch_lda:Iteration: 37\n",
      "INFO:mini_batch_lda:Ro: 0.0544719751074\n",
      "INFO:mini_batch_lda:Iteration: 38\n",
      "INFO:mini_batch_lda:Ro: 0.0533517063913\n",
      "INFO:mini_batch_lda:Iteration: 39\n",
      "INFO:mini_batch_lda:Ro: 0.0522819776296\n",
      "INFO:mini_batch_lda:Iteration: 40\n",
      "INFO:mini_batch_lda:Ro: 0.0512593287384\n",
      "INFO:mini_batch_lda:Iteration: 41\n",
      "INFO:mini_batch_lda:Ro: 0.0502806134516\n",
      "INFO:mini_batch_lda:Iteration: 42\n",
      "INFO:mini_batch_lda:Ro: 0.0493429642056\n",
      "INFO:mini_batch_lda:Iteration: 43\n",
      "INFO:mini_batch_lda:Ro: 0.0484437616652\n",
      "INFO:mini_batch_lda:Iteration: 44\n",
      "INFO:mini_batch_lda:Ro: 0.0475806081853\n",
      "INFO:mini_batch_lda:Iteration: 45\n",
      "INFO:mini_batch_lda:Ro: 0.0467513046263\n",
      "INFO:mini_batch_lda:Iteration: 46\n",
      "INFO:mini_batch_lda:Ro: 0.0459538300375\n",
      "INFO:mini_batch_lda:Iteration: 47\n",
      "INFO:mini_batch_lda:Ro: 0.0451863237999\n",
      "INFO:mini_batch_lda:Iteration: 48\n",
      "INFO:mini_batch_lda:Ro: 0.0444470698874\n",
      "INFO:mini_batch_lda:Iteration: 49\n",
      "INFO:mini_batch_lda:Ro: 0.0437344829577\n",
      "INFO:mini_batch_lda:Iteration: 50\n",
      "INFO:mini_batch_lda:Ro: 0.043047096028\n",
      "INFO:mini_batch_lda:Iteration: 51\n",
      "INFO:mini_batch_lda:Ro: 0.0423835495277\n",
      "INFO:mini_batch_lda:Iteration: 52\n",
      "INFO:mini_batch_lda:Ro: 0.0417425815524\n",
      "INFO:mini_batch_lda:Iteration: 53\n",
      "INFO:mini_batch_lda:Ro: 0.0411230191652\n",
      "INFO:mini_batch_lda:Iteration: 54\n",
      "INFO:mini_batch_lda:Ro: 0.0405237706188\n",
      "INFO:mini_batch_lda:Iteration: 55\n",
      "INFO:mini_batch_lda:Ro: 0.0399438183829\n",
      "INFO:mini_batch_lda:Iteration: 56\n",
      "INFO:mini_batch_lda:Ro: 0.0393822128836\n",
      "INFO:mini_batch_lda:Iteration: 57\n",
      "INFO:mini_batch_lda:Ro: 0.0388380668695\n",
      "INFO:mini_batch_lda:Iteration: 58\n",
      "INFO:mini_batch_lda:Ro: 0.0383105503327\n",
      "INFO:mini_batch_lda:Iteration: 59\n",
      "INFO:mini_batch_lda:Ro: 0.0377988859211\n",
      "INFO:mini_batch_lda:Iteration: 60\n",
      "INFO:mini_batch_lda:Ro: 0.0373023447888\n",
      "INFO:mini_batch_lda:Iteration: 61\n",
      "INFO:mini_batch_lda:Ro: 0.0368202428339\n",
      "INFO:mini_batch_lda:Iteration: 62\n",
      "INFO:mini_batch_lda:Ro: 0.0363519372845\n",
      "INFO:mini_batch_lda:Iteration: 63\n",
      "INFO:mini_batch_lda:Ro: 0.0358968235937\n",
      "INFO:mini_batch_lda:Iteration: 64\n",
      "INFO:mini_batch_lda:Ro: 0.0354543326123\n",
      "INFO:mini_batch_lda:Iteration: 65\n",
      "INFO:mini_batch_lda:Ro: 0.0350239280111\n",
      "INFO:mini_batch_lda:Iteration: 66\n",
      "INFO:mini_batch_lda:Ro: 0.034605103925\n",
      "INFO:mini_batch_lda:Iteration: 67\n",
      "INFO:mini_batch_lda:Ro: 0.0341973828006\n",
      "INFO:mini_batch_lda:Iteration: 68\n",
      "INFO:mini_batch_lda:Ro: 0.0338003134226\n",
      "INFO:mini_batch_lda:Iteration: 69\n",
      "INFO:mini_batch_lda:Ro: 0.0334134691061\n",
      "INFO:mini_batch_lda:Iteration: 70\n",
      "INFO:mini_batch_lda:Ro: 0.0330364460353\n",
      "INFO:mini_batch_lda:Iteration: 71\n",
      "INFO:mini_batch_lda:Ro: 0.0326688617368\n",
      "INFO:mini_batch_lda:Iteration: 72\n",
      "INFO:mini_batch_lda:Ro: 0.0323103536745\n",
      "INFO:mini_batch_lda:Iteration: 73\n",
      "INFO:mini_batch_lda:Ro: 0.0319605779545\n",
      "INFO:mini_batch_lda:Iteration: 74\n",
      "INFO:mini_batch_lda:Ro: 0.0316192081304\n",
      "INFO:mini_batch_lda:Iteration: 75\n",
      "INFO:mini_batch_lda:Ro: 0.0312859340997\n",
      "INFO:mini_batch_lda:Iteration: 76\n",
      "INFO:mini_batch_lda:Ro: 0.0309604610838\n",
      "INFO:mini_batch_lda:Iteration: 77\n",
      "INFO:mini_batch_lda:Ro: 0.030642508684\n",
      "INFO:mini_batch_lda:Iteration: 78\n",
      "INFO:mini_batch_lda:Ro: 0.0303318100061\n",
      "INFO:mini_batch_lda:Iteration: 79\n",
      "INFO:mini_batch_lda:Ro: 0.0300281108495\n",
      "INFO:mini_batch_lda:Iteration: 80\n",
      "INFO:mini_batch_lda:Ro: 0.0297311689541\n",
      "INFO:mini_batch_lda:Iteration: 81\n",
      "INFO:mini_batch_lda:Ro: 0.0294407533\n",
      "INFO:mini_batch_lda:Iteration: 82\n",
      "INFO:mini_batch_lda:Ro: 0.0291566434574\n",
      "INFO:mini_batch_lda:Iteration: 83\n",
      "INFO:mini_batch_lda:Ro: 0.02887862898\n",
      "INFO:mini_batch_lda:Iteration: 84\n",
      "INFO:mini_batch_lda:Ro: 0.0286065088411\n",
      "INFO:mini_batch_lda:Iteration: 85\n",
      "INFO:mini_batch_lda:Ro: 0.0283400909068\n",
      "INFO:mini_batch_lda:Iteration: 86\n",
      "INFO:mini_batch_lda:Ro: 0.0280791914453\n",
      "INFO:mini_batch_lda:Iteration: 87\n",
      "INFO:mini_batch_lda:Ro: 0.0278236346674\n",
      "INFO:mini_batch_lda:Iteration: 88\n",
      "INFO:mini_batch_lda:Ro: 0.0275732522983\n",
      "INFO:mini_batch_lda:Iteration: 89\n",
      "INFO:mini_batch_lda:Ro: 0.0273278831761\n",
      "INFO:mini_batch_lda:Iteration: 90\n",
      "INFO:mini_batch_lda:Ro: 0.0270873728761\n",
      "INFO:mini_batch_lda:Iteration: 91\n",
      "INFO:mini_batch_lda:Ro: 0.0268515733591\n",
      "INFO:mini_batch_lda:Iteration: 92\n",
      "INFO:mini_batch_lda:Ro: 0.0266203426413\n",
      "INFO:mini_batch_lda:Iteration: 93\n",
      "INFO:mini_batch_lda:Ro: 0.026393544485\n",
      "INFO:mini_batch_lda:Iteration: 94\n",
      "INFO:mini_batch_lda:Ro: 0.0261710481075\n",
      "INFO:mini_batch_lda:Iteration: 95\n",
      "INFO:mini_batch_lda:Ro: 0.0259527279087\n",
      "INFO:mini_batch_lda:Iteration: 96\n",
      "INFO:mini_batch_lda:Ro: 0.0257384632137\n",
      "INFO:mini_batch_lda:Iteration: 97\n",
      "INFO:mini_batch_lda:Ro: 0.025528138032\n",
      "INFO:mini_batch_lda:Iteration: 98\n",
      "INFO:mini_batch_lda:Ro: 0.0253216408296\n",
      "INFO:mini_batch_lda:Iteration: 99\n",
      "INFO:mini_batch_lda:Ro: 0.0251188643151\n",
      "INFO:mini_batch_lda:Iteration: 100\n",
      "INFO:mini_batch_lda:Ro: 0.0249197052378\n",
      "INFO:mini_batch_lda:Iteration: 101\n",
      "INFO:mini_batch_lda:Ro: 0.0247240641974\n",
      "INFO:mini_batch_lda:Iteration: 102\n",
      "INFO:mini_batch_lda:Ro: 0.0245318454645\n",
      "INFO:mini_batch_lda:Iteration: 103\n",
      "INFO:mini_batch_lda:Ro: 0.0243429568107\n",
      "INFO:mini_batch_lda:Iteration: 104\n",
      "INFO:mini_batch_lda:Ro: 0.0241573093489\n",
      "INFO:mini_batch_lda:Iteration: 105\n",
      "INFO:mini_batch_lda:Ro: 0.0239748173813\n",
      "INFO:mini_batch_lda:Iteration: 106\n",
      "INFO:mini_batch_lda:Ro: 0.0237953982564\n",
      "INFO:mini_batch_lda:Iteration: 107\n",
      "INFO:mini_batch_lda:Ro: 0.0236189722338\n",
      "INFO:mini_batch_lda:Iteration: 108\n",
      "INFO:mini_batch_lda:Ro: 0.0234454623554\n",
      "INFO:mini_batch_lda:Iteration: 109\n",
      "INFO:mini_batch_lda:Ro: 0.023274794324\n",
      "INFO:mini_batch_lda:Iteration: 110\n",
      "INFO:mini_batch_lda:Ro: 0.0231068963886\n",
      "INFO:mini_batch_lda:Iteration: 111\n",
      "INFO:mini_batch_lda:Ro: 0.0229416992343\n",
      "INFO:mini_batch_lda:Iteration: 112\n",
      "INFO:mini_batch_lda:Ro: 0.0227791358797\n",
      "INFO:mini_batch_lda:Iteration: 113\n",
      "INFO:mini_batch_lda:Ro: 0.0226191415778\n",
      "INFO:mini_batch_lda:Iteration: 114\n",
      "INFO:mini_batch_lda:Ro: 0.0224616537229\n",
      "INFO:mini_batch_lda:Iteration: 115\n",
      "INFO:mini_batch_lda:Ro: 0.0223066117621\n",
      "INFO:mini_batch_lda:Iteration: 116\n",
      "INFO:mini_batch_lda:Ro: 0.022153957111\n",
      "INFO:mini_batch_lda:Iteration: 117\n",
      "INFO:mini_batch_lda:Ro: 0.0220036330731\n",
      "INFO:mini_batch_lda:Iteration: 118\n",
      "INFO:mini_batch_lda:Ro: 0.0218555847641\n",
      "INFO:mini_batch_lda:Iteration: 119\n",
      "INFO:mini_batch_lda:Ro: 0.0217097590391\n",
      "INFO:mini_batch_lda:Iteration: 120\n",
      "INFO:mini_batch_lda:Ro: 0.0215661044238\n",
      "INFO:mini_batch_lda:Iteration: 121\n",
      "INFO:mini_batch_lda:Ro: 0.0214245710482\n",
      "INFO:mini_batch_lda:Iteration: 122\n",
      "INFO:mini_batch_lda:Ro: 0.0212851105845\n",
      "INFO:mini_batch_lda:Iteration: 123\n",
      "INFO:mini_batch_lda:Ro: 0.0211476761869\n",
      "INFO:mini_batch_lda:Iteration: 124\n",
      "INFO:mini_batch_lda:Ro: 0.0210122224352\n",
      "INFO:mini_batch_lda:Iteration: 125\n",
      "INFO:mini_batch_lda:Ro: 0.0208787052798\n",
      "INFO:mini_batch_lda:Iteration: 126\n",
      "INFO:mini_batch_lda:Ro: 0.0207470819903\n",
      "INFO:mini_batch_lda:Iteration: 127\n",
      "INFO:mini_batch_lda:Ro: 0.0206173111058\n",
      "INFO:mini_batch_lda:Iteration: 128\n",
      "INFO:mini_batch_lda:Ro: 0.0204893523878\n",
      "INFO:mini_batch_lda:Iteration: 129\n",
      "INFO:mini_batch_lda:Ro: 0.0203631667747\n",
      "INFO:mini_batch_lda:Iteration: 130\n",
      "INFO:mini_batch_lda:Ro: 0.020238716339\n",
      "INFO:mini_batch_lda:Iteration: 131\n",
      "INFO:mini_batch_lda:Ro: 0.0201159642459\n",
      "INFO:mini_batch_lda:Iteration: 132\n",
      "INFO:mini_batch_lda:Ro: 0.0199948747138\n",
      "INFO:mini_batch_lda:Iteration: 133\n",
      "INFO:mini_batch_lda:Ro: 0.0198754129766\n",
      "INFO:mini_batch_lda:Iteration: 134\n",
      "INFO:mini_batch_lda:Ro: 0.0197575452474\n",
      "INFO:mini_batch_lda:Iteration: 135\n",
      "INFO:mini_batch_lda:Ro: 0.0196412386841\n",
      "INFO:mini_batch_lda:Iteration: 136\n",
      "INFO:mini_batch_lda:Ro: 0.0195264613563\n",
      "INFO:mini_batch_lda:Iteration: 137\n",
      "INFO:mini_batch_lda:Ro: 0.0194131822135\n",
      "INFO:mini_batch_lda:Iteration: 138\n",
      "INFO:mini_batch_lda:Ro: 0.0193013710544\n",
      "INFO:mini_batch_lda:Iteration: 139\n",
      "INFO:mini_batch_lda:Ro: 0.0191909984985\n",
      "INFO:mini_batch_lda:Iteration: 140\n",
      "INFO:mini_batch_lda:Ro: 0.0190820359572\n",
      "INFO:mini_batch_lda:Iteration: 141\n",
      "INFO:mini_batch_lda:Ro: 0.0189744556078\n",
      "INFO:mini_batch_lda:Iteration: 142\n",
      "INFO:mini_batch_lda:Ro: 0.0188682303675\n",
      "INFO:mini_batch_lda:Iteration: 143\n",
      "INFO:mini_batch_lda:Ro: 0.0187633338683\n",
      "INFO:mini_batch_lda:Iteration: 144\n",
      "INFO:mini_batch_lda:Ro: 0.0186597404342\n",
      "INFO:mini_batch_lda:Iteration: 145\n",
      "INFO:mini_batch_lda:Ro: 0.0185574250577\n"
     ]
    }
   ],
   "source": [
    "D = len(words_by_doc)  # number of documents\n",
    "N = 60 # words per doc\n",
    "K = 30  # number of topics\n",
    "V = len(vocab)  # vocabulary size\n",
    "minibatch_size = 128\n",
    "\n",
    "# For alpha and eta values I follow suggestion from Griffiths and Steyvers (2004)\n",
    "lda = MiniBatchLDA(K, D, N, V, 50.0/K, 0.1, words_by_doc, minibatch_size)\n",
    "lda.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lamb_k in enumerate(lda.lamb):\n",
    "    print_md('Topic {}'.format(i), bold=True)\n",
    "    np.random.seed(42)\n",
    "    words_distribution = np.random.dirichlet(lamb_k)\n",
    "    #print words_distribution.argsort()[::-1][:10]\n",
    "    #words_distribution.sort()\n",
    "    #print words_distribution[::-1][:10]\n",
    "    print_words(words_distribution.argsort()[::-1][:20], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_k_topics_for_doc(lda, 2000, 10, words_by_doc, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = len(words_by_doc)  # number of documents\n",
    "N = 60 # words per doc\n",
    "K = 60  # number of topics\n",
    "V = len(vocab)  # vocabulary size\n",
    "\n",
    "# For alpha and eta values I follow recommendation from Griffiths and Steyvers (2004)\n",
    "lda = OnlineLDA(K, D, N, V, 50.0/K, 0.1, words_by_doc)\n",
    "lda.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, lamb_k in enumerate(lda.lamb):\n",
    "    print_md('Topic {}'.format(i), bold=True)\n",
    "    np.random.seed(42)\n",
    "    words_distribution = np.random.dirichlet(lamb_k)\n",
    "    #print words_distribution.argsort()[::-1][:10]\n",
    "    #words_distribution.sort()\n",
    "    #print words_distribution[::-1][:10]\n",
    "    print_words(words_distribution.argsort()[::-1][:20], vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
